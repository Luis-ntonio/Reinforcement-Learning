{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cells: [(0, 0), (0, 2), (0, 3), (1, 0), (1, 1), (1, 3), (2, 1), (2, 2), (3, 0), (3, 1), (3, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258675/2566880433.py:104: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  state_batch = torch.tensor(state_batch, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best placement: Item 2 at (3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# Environment Parameters\n",
    "rows, cols = 4, 4  # Matrix size\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay = 0.995\n",
    "episodes = 5000\n",
    "batch_size = 32\n",
    "memory_size = 1000\n",
    "\n",
    "# Matrix of availability (0 = free, 1 = occupied)\n",
    "avail_matrix = np.array([\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Matrix of weights (higher values indicate higher cost to place an item)\n",
    "weight_matrix = np.array([\n",
    "    [3, 9, 2, 1],\n",
    "    [5, 4, 8, 2],\n",
    "    [6, 3, 2, 7],\n",
    "    [1, 5, 9, 3]\n",
    "])\n",
    "\n",
    "# Items to be placed and their values\n",
    "items = [10, 15, 8]  # Example values for m = 3 items\n",
    "m = len(items)  # Number of items\n",
    "available_cells = [(i, j) for i in range(rows) for j in range(cols) if avail_matrix[i, j] == 0]\n",
    "n = len(available_cells)  # Number of available cells\n",
    "\n",
    "# Convert (item_id, cell index) into an action index\n",
    "def state_to_index(item_id, cell_index):\n",
    "    return item_id * n + cell_index\n",
    "\n",
    "# Convert action index back to (item_id, i, j)\n",
    "def index_to_state(index):\n",
    "    item_id = index // n\n",
    "    cell_index = index % n\n",
    "    return item_id, available_cells[cell_index]\n",
    "\n",
    "# Reward function: Penalizes weight, rewards item value, avoids occupied cells\n",
    "def reward_function(i, j, item_id):\n",
    "    if avail_matrix[i, j] == 1:\n",
    "        return -100  # Penalize occupied cells\n",
    "    return items[item_id] - weight_matrix[i, j]  # Reward item value, penalize weight\n",
    "\n",
    "# Neural Network for DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize Networks\n",
    "state_size = n + m  # Available cells + items placed\n",
    "action_size = m * n  # Choosing (item, cell)\n",
    "policy_net = DQN(state_size, action_size)\n",
    "target_net = DQN(state_size, action_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Select action (item, cell) using Îµ-greedy\n",
    "def select_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        item_id = random.choice(range(m))  # Select an item\n",
    "        cell_index = random.choice(range(n))  # Select an available cell\n",
    "        return item_id, available_cells[cell_index]\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state_tensor)\n",
    "        best_action = torch.argmax(q_values).item()\n",
    "        return index_to_state(best_action)\n",
    "\n",
    "# Train DQN using Replay Buffer\n",
    "def train_dqn():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = zip(*batch)\n",
    "\n",
    "    state_batch = torch.tensor(state_batch, dtype=torch.float32)\n",
    "    action_batch = [state_to_index(item_id, available_cells.index((i, j))) for item_id, (i, j) in action_batch]\n",
    "    action_batch = torch.tensor(action_batch, dtype=torch.int64)\n",
    "    reward_batch = torch.tensor(reward_batch, dtype=torch.float32)\n",
    "    next_state_batch = torch.tensor(next_state_batch, dtype=torch.float32)\n",
    "\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "    expected_q_values = reward_batch + (discount_factor * next_q_values)\n",
    "\n",
    "    loss = F.mse_loss(q_values, expected_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    if len(available_cells) < m:\n",
    "        break  # Not enough space for all items\n",
    "\n",
    "    state = np.zeros(state_size)  # Initial state: empty matrix, no items placed\n",
    "    placed_items = set()  # Track placed items\n",
    "\n",
    "    for step in range(m):  # We must place m items\n",
    "        item_id, (i, j) = select_action(state, exploration_rate)\n",
    "\n",
    "        # Ensure item is not placed twice\n",
    "        if item_id in placed_items:\n",
    "            continue\n",
    "        placed_items.add(item_id)\n",
    "\n",
    "        reward = reward_function(i, j, item_id)\n",
    "\n",
    "        # Update state representation\n",
    "        next_state = state.copy()\n",
    "        next_state[available_cells.index((i, j))] = 1\n",
    "\n",
    "        # Save experience to memory\n",
    "        memory.append((state, (item_id, (i, j)), reward, next_state))\n",
    "\n",
    "        # Train model\n",
    "        train_dqn()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay exploration rate\n",
    "    exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "\n",
    "    # Update target network every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Find the best placement\n",
    "state = np.zeros(state_size)\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "q_values = policy_net(state_tensor)\n",
    "best_action = torch.argmax(q_values).item()\n",
    "best_item_id, best_cell = index_to_state(best_action)\n",
    "\n",
    "print(f\"Best placement: Item {best_item_id} at {best_cell}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
