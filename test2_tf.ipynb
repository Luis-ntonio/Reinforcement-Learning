{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 13:09:10.623363: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-11 13:09:10.630433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741716550.639258  405343 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741716550.641746  405343 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 13:09:10.650914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import itertools\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "class AnaquelEnv:\n",
    "    def __init__(self, df, rows=3, cols=7):\n",
    "        self.df = df.copy()\n",
    "        self.df_iterations = df.copy()\n",
    "        self.weight_matrix = np.array([\n",
    "            [5.5, 4.7, 3.5, 3.0, 2.0, 1.3, 1.0, 1.0, 1.3, 2.0, 3.0, 3.5, 4.7, 5.5, 5.5, 4.7, 3.5, 3.0, 2.0, 1.3, 1.0, 1.0, 1.3, 2.0, 3.0, 3.5, 4.7, 5.5, 5.5, 4.7, 3.5, 3.0, 2.0, 1.3, 1.0, 1.0, 1.3, 2.0, 3.0, 3.5, 4.7, 5.5],\n",
    "            [5.0, 4.3, 3.0, 2.7, 1.6, 1.0, 0.7, 0.7, 1.0, 1.6, 2.7, 3.0, 4.3, 5.0, 5.0, 4.3, 3.0, 2.7, 1.6, 1.0, 0.7, 0.7, 1.0, 1.6, 2.7, 3.0, 4.3, 5.0, 5.0, 4.3, 3.0, 2.7, 1.6, 1.0, 0.7, 0.7, 1.0, 1.6, 2.7, 3.0, 4.3, 5.0],\n",
    "            [4.3, 3.6, 2.5, 2.0, 1.3, 0.7, 0.5, 0.5, 0.7, 1.3, 2.0, 2.5, 3.6, 4.3, 4.3, 3.6, 2.5, 2.0, 1.3, 0.7, 0.5, 0.5, 0.7, 1.3, 2.0, 2.5, 3.6, 4.3, 4.3, 3.6, 2.5, 2.0, 1.3, 0.7, 0.5, 0.5, 0.7, 1.3, 2.0, 2.5, 3.6, 4.3],\n",
    "            [9.0, 7.7, 6.5, 5.5, 5.0, 4.3, 4.0, 4.0, 4.3, 5.0, 5.5, 6.5, 7.7, 9.0, 9.0, 7.7, 6.5, 5.5, 5.0, 4.3, 4.0, 4.0, 4.3, 5.0, 5.5, 6.5, 7.7, 9.0, 9.0, 7.7, 6.5, 5.5, 5.0, 4.3, 4.0, 4.0, 4.3, 5.0, 5.5, 6.5, 7.7, 9.0],\n",
    "            [9.8, 8.5, 7.0, 6.0, 5.5, 4.7, 4.3, 4.3, 4.7, 5.5, 6.0, 7.0, 8.5, 9.8, 9.8, 8.5, 7.0, 6.0, 5.5, 4.7, 4.3, 4.3, 4.7, 5.5, 6.0, 7.0, 8.5, 9.8, 9.8, 8.5, 7.0, 6.0, 5.5, 4.7, 4.3, 4.3, 4.7, 5.5, 6.0, 7.0, 8.5, 9.8],\n",
    "            [10.5, 9.0, 7.7, 6.5, 6.0, 5.0, 4.7, 4.7, 5.0, 6.0, 6.5, 7.7, 9.0, 10.5, 10.5, 9.0, 7.7, 6.5, 6.0, 5.0, 4.7, 4.7, 5.0, 6.0, 6.5, 7.7, 9.0, 10.5, 10.5, 9.0, 7.7, 6.5, 6.0, 5.0, 4.7, 4.7, 5.0, 6.0, 6.5, 7.7, 9.0, 10.5]\n",
    "        ])\n",
    "        self.rows = self.weight_matrix.shape[0]\n",
    "        self.cols = self.weight_matrix.shape[1]\n",
    "        # Matrix of weights (higher values indicate higher cost to place an item)\n",
    "\n",
    "        self.avail_matrix = np.zeros(self.weight_matrix.shape)\n",
    "        self.products_id = np.zeros(self.weight_matrix.shape)\n",
    "        self.products_id.fill(-1)\n",
    "        \n",
    "        # Mapping product IDs to indexes for one-hot encoding\n",
    "        unique_products = df['PRODUCTO'].unique()\n",
    "        self.product_id_to_index = {pid: idx for idx, pid in enumerate(unique_products)}\n",
    "        self.num_products = len(unique_products)\n",
    "\n",
    "        # State representations\n",
    "        self.state_quantities = np.zeros(self.weight_matrix.shape)  \n",
    "        self.state_space = (self.rows * self.cols) + self.num_products  # Total possible states\n",
    "        self.action_space = rows * cols * self.num_products # Total possible placements\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for new episode\"\"\"\n",
    "        self.df_iterations = self.df.copy()\n",
    "        self.state_quantities.fill(0)\n",
    "        self.avail_matrix.fill(0)\n",
    "        self.products_id.fill(-1)\n",
    "        return self.state_quantities, self.products_id\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Perform an action and return next state, reward, and done flag\"\"\"\n",
    "        item = self.df_iterations.sample()\n",
    "        product_id = item['PRODUCTO'].values[0]\n",
    "        quantity = item['UNDESTIMADAS'].values[0]\n",
    "\n",
    "        if product_id not in self.product_id_to_index:\n",
    "            raise ValueError(f\"Product ID {product_id} not found in mapping\")\n",
    "\n",
    "        row, col = action\n",
    "\n",
    "        if self.avail_matrix[row, col] == -1:  # If cell is empty\n",
    "            self.products_id[row, col] = product_id\n",
    "            self.state_quantities[row, col] = quantity\n",
    "            self.avail_matrix[row, col] = 1\n",
    "            self.df_iterations = self.df_iterations[self.df_iterations['PRODUCTO'] != product_id]\n",
    "\n",
    "        reward = self.compute_reward()\n",
    "        done = self.is_done()\n",
    "\n",
    "        return self.state_quantities, self.avail_matrix, reward, done\n",
    "\n",
    "\n",
    "    def compute_reward(self, row, col):\n",
    "        \"\"\"Reward function: balance zones, prioritize high-demand items in front\"\"\"\n",
    "        if self.avail_matrix[row, col] == -1:\n",
    "            return -5000\n",
    "        return self.state_quantities[row, col] * self.weight_matrix[row, col]\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.all(self.df_iterations.empty)  # Done when all cells are filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (65174245) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "file_path = 'productos_anaquel.xls'\n",
    "df_ = []\n",
    "i = 1\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler('log.txt')\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.info('Reading file...')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        df_.append(pd.read_excel(file_path, sheet_name=f\"Sheet {i}\"))\n",
    "        i += 1\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_ = pd.concat(df_, ignore_index=True)\n",
    "df = df_[df_['ANAQUEL'].str.startswith('C', na=False)]\n",
    "df = df[df['CAMPA'] == 201416]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc4 = tf.keras.layers.Dense(output_dim, activation=None)  # No activation, raw Q-values\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: A Q-Network that returns Q-values for a given state.\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes (sess, observation, epsilon) and returns\n",
    "        probabilities for each action as a numpy array of length nA.\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA  # Uniform exploration probability\n",
    "        q_values = estimator(tf.expand_dims(observation, axis=0))[0].numpy()  # Get Q-values\n",
    "        best_action = np.argmax(q_values)  # Choose best action\n",
    "        A[best_action] += (1.0 - epsilon)  # Favor best action\n",
    "        return A\n",
    "    return policy_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(q_network, target_q_network):\n",
    "    target_q_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env: AnaquelEnv,\n",
    "                    q_estimator: QNetwork,\n",
    "                    target_estimator: QNetwork,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=5000,\n",
    "                    replay_memory_init_size=1000,\n",
    "                    update_target_estimator_every=500,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=50000,\n",
    "                    batch_size=32):\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    replay_memory = deque(maxlen=replay_memory_size)\n",
    "    rewards_list = []\n",
    "\n",
    "    # Create directories for saving models\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model.weights.h5\")\n",
    "\n",
    "    # Create epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # Define ε-greedy policy\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, env.action_space)\n",
    "\n",
    "    # Populate replay memory with initial random experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    env.reset()\n",
    "    # to be understand\n",
    "    state = np.zeros(env.state_space)\n",
    "\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(i, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        row, col = np.unravel_index(action, (env.rows, env.cols))\n",
    "\n",
    "        next_state_quantities, avail_matrix, reward, done = env.step((row, col))\n",
    "        next_state = avail_matrix\n",
    "\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            state = np.zeros(env.state_space)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    print(\"Replay memory initialized.\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = np.zeros(env.state_space)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            epsilon = epsilons[min(step_count, epsilon_decay_steps - 1)]\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            row, col = np.unravel_index(action, (env.rows, env.cols))\n",
    "\n",
    "            next_state_quantities, avail_matrix, reward, done = env.step((row, col))\n",
    "            next_state = avail_matrix\n",
    "            \n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "            if len(replay_memory) >= batch_size:\n",
    "                batch = random.sample(replay_memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
    "                actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "                rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)\n",
    "                dones = tf.convert_to_tensor(np.array(dones, dtype=np.float32), dtype=tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = q_estimator(states)\n",
    "                    q_values = tf.gather(q_values, actions, batch_dims=1)\n",
    "\n",
    "                    next_q_values = target_estimator(next_states)\n",
    "                    max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "                    targets = rewards + discount_factor * max_next_q_values * (1 - dones)\n",
    "                    \n",
    "                    loss = tf.keras.losses.MSE(targets, q_values)\n",
    "\n",
    "                grads = tape.gradient(loss, q_estimator.trainable_variables)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "                optimizer.apply_gradients(zip(grads, q_estimator.trainable_variables))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards_list.append(total_reward)\n",
    "\n",
    "        # Update target network every few episodes\n",
    "        if episode % update_target_estimator_every == 0:\n",
    "            target_estimator.set_weights(q_estimator.get_weights())\n",
    "\n",
    "        # Save model checkpoint\n",
    "        q_estimator.save_weights(checkpoint_path)\n",
    "\n",
    "        print(f\"Episode {episode+1}, Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    return rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"./experiments\"\n",
    "\n",
    "checkpoint_path = os.path.join(experiment_dir, 'checkpoints', \"model.weights.h5\")\n",
    "\n",
    "# Initialize environment and state processor\n",
    "env = AnaquelEnv(df)\n",
    "\n",
    "# Get input and output dimensions\n",
    "num_products = env.num_products\n",
    "input_dim = (env.rows * env.cols) * (1 + num_products)\n",
    "output_dim = env.action_space\n",
    "\n",
    "# Create Q-networks (online & target)\n",
    "q_network = QNetwork(input_dim, output_dim)\n",
    "target_q_network = QNetwork(input_dim, output_dim)\n",
    "target_q_network.set_weights(q_network.get_weights())  # Sync weights initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found! Train the model first.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading saved weights...\")\n",
    "    dummy_input = tf.random.uniform((1, input_dim))  # Create dummy input\n",
    "    q_network(dummy_input)  # Forward pass to initialize model\n",
    "    target_q_network(dummy_input)\n",
    "\n",
    "    # Now load weights\n",
    "    q_network.load_weights(checkpoint_path)\n",
    "    target_q_network.load_weights(checkpoint_path)\n",
    "else:\n",
    "    print(\"No saved model found! Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741716585.489997  405343 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13912 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "index 7447 is out of bounds for array with size 252",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rewards_list \u001b[38;5;241m=\u001b[39m \u001b[43mdeep_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m policy(sess, state, epsilons[\u001b[38;5;28mmin\u001b[39m(i, epsilon_decay_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     40\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(action_probs)), p\u001b[38;5;241m=\u001b[39maction_probs)\n\u001b[0;32m---> 41\u001b[0m row, col \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munravel_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m next_state_quantities, avail_matrix, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep((row, col))\n\u001b[1;32m     44\u001b[0m next_state \u001b[38;5;241m=\u001b[39m avail_matrix\n",
      "\u001b[0;31mValueError\u001b[0m: index 7447 is out of bounds for array with size 252"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "rewards_list = deep_q_learning(None, env, q_network, target_q_network, num_episodes=500, experiment_dir=experiment_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(env, q_network, state_processor, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Runs the trained agent in the environment without exploration (ε = 0).\n",
    "\n",
    "    Args:\n",
    "        env: The environment to test in.\n",
    "        q_network: The trained Q-Network.\n",
    "        state_processor: Processes environment states into model-compatible format.\n",
    "        num_episodes: Number of episodes to test.\n",
    "\n",
    "    Returns:\n",
    "        A list of total rewards for each episode.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state_quantities, state_products_onehot = env.reset()\n",
    "        state = state_processor.process(state_quantities, state_products_onehot)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            # Get action from trained model (greedy policy, no exploration)\n",
    "            q_values = q_network(tf.expand_dims(state, axis=0))[0].numpy()\n",
    "            action = np.argmax(q_values)  # Choose best action\n",
    "\n",
    "            # Convert action index to (zone, anaquel, row, col)\n",
    "            zone, anaquel, row, col = np.unravel_index(action, (env.zones, env.anaqueles_per_zone, env.rows, env.cols))\n",
    "\n",
    "            # Take step in the environment\n",
    "            next_state_quantities, next_state_products_onehot, reward, done = env.step((zone, anaquel, row, col))\n",
    "            next_state = state_processor.process(next_state_quantities, next_state_products_onehot)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Test Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    return total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = test_trained_model(env, q_network, state_processor, num_episodes=10)\n",
    "print(\"Average Reward over 10 Episodes:\", np.mean(test_rewards))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
